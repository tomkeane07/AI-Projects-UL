{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Etivity_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d9-nEXis_gaV"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-nEXis_gaV"
      },
      "source": [
        "# Etivity Assignment 3: Object co-localisation\n",
        "In this assignment we will use the [Deep Descriptor Transforming (DDT) Technique](https://arxiv.org/pdf/1707.06397.pdf) to perform object co-localisation. The object localisation task involves finding the location of the primary object in an image (same object as image classification result). In the object co-localisation task the goal is to find the objects in images all of the same class. As detailed in the DDT paper, this can be achieved in a unsuperivsed manner from the feature map outputs of a pre-trained network. (In the image: Green bounding box is ground truth, Red boxes are obtained from DDT)\n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/etivity3_assignment_img.JPG?raw=true)\n",
        "\n",
        "This technique (like Class activiation mapping) demonstrates that spatial information is contained with Deep Neural Networks trained for classification. This information can be used to generate region proposals or for direct object detection. The  unsupervised technique proposed in this paper is useful as it avoids the necessity of having bounding box information to adapt the pre-trained network for localisation.\n",
        "\n",
        "In this Assignment you will perform object co-localisation using the stanford dogs dataset and the pre-trained VGG-16 network. This Jupyter notebook loads the stanford dogs dataset and also sets up the pre-trained Keras VGG-16 model so that the features of the convolutional layer are output. (Output layers of the model can be changed if required).  \n",
        "\n",
        "In section 4. of the notebook you will implement the DDT algorithm and demonstrate prediction of bounding boxes. (Ground truth bounding boxes are available for comparison with your prediction).\n",
        "\n",
        "You may find the following information useful:\n",
        "\n",
        "*   [Sklearn PCA Functions](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "* [Tensorflow resize](https://www.tensorflow.org/api_docs/python/tf/image/resize) (interpolation function)\n",
        "*   OpenCV Connected Components (PyImageSearch example [link text](https://www.pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/)) Note that if you want to use OpenCv on the output tensors then it will be necessary to need to convert the tensor to a numpy array uisng `.numpy()` and also ensure that have the numpy array in uint8 format `.astype(np.uint8)`\n",
        "* If you wish to display any of the images from the dataset, please note that they have been processed prior to input to the network. In order to unprocess the image for display, please use: `helper.unprocess_image(img)`\n",
        "\n",
        "In order to obtain an exemplary grade you will be required to implement the DDT+ algorithm. This will involve use of additional feature map information to refine the position of the bounding box compared to the basic approach (as shown in the images below). You will implement the DDT+ algorithm on the [Pascale Visual Object Classes Challenge](https://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf) \"Sheep\" class. This is more challenging than the stanford dogs dataset, as objects appear at different scales. Note that the DDT+ approach also causes failures in bounding boxes for many images, you can comment on this in your notebook. \n",
        "\n",
        "Image: DDT+ Performance (Cyan: DDT+ bouding boxes, Red DDT: bounding boxes, Green: Ground Truth) \n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/etivity3b_assignment.JPG?raw=true)\n",
        "\n",
        "* Note: Completed list of VoC Classes 0-airplane, 1-bicycle, 2-bird, 3-boat, 4-bottle, 5-bus, 6-car, 7-cat, 8-chair, 9-cow, 10-dining table, 11-dog, 12-horse, 13-motorbike, 14-person, 15-potted plant, 16-sheep, 17-sofa, 18-train, 19-TV/monitor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_rx0kwv8pRA"
      },
      "source": [
        "## 1. HouseKeeping\n",
        " Clone Repository & Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXb5G3eUQe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26bc016-77bb-471d-dba3-71bd232122e1"
      },
      "source": [
        "# Clone repository to gain access to helper.py\n",
        "!git clone https://github.com/tonyscan6003/CE6003.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CE6003' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uKx_naKbo9c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import CE6003.python.helper as helper\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZmPHMgRWq9M"
      },
      "source": [
        "# Global Variables\n",
        "HW_trg = helper.myList[0]      # Target Input Image size\n",
        "batch_size = helper.myList[1]  # Batch Size\n",
        "data_set = [\"stanford_dogs\"]   # Dataset (and add class integer to list for VoC))\n",
        "#data_set = [\"voc\",16]    # VoC Dataset with Sheep Class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTKCLOSsVkt"
      },
      "source": [
        "## 2A. Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWIHP-Ja9WPY"
      },
      "source": [
        "def get_data(data_set):\n",
        "  src_train_dataset,info = tfds.load(data_set[0],split='train',with_info=True)\n",
        "  src_test_dataset,info  = tfds.load(data_set[0],split='test',with_info=True)\n",
        "  return src_train_dataset, src_test_dataset,info\n",
        "\n",
        "  \n",
        "try:\n",
        "  src_train_dataset_0, src_test_dataset_0,info_0\n",
        "except:\n",
        "  src_train_dataset_0, src_test_dataset_0,info_0 = get_data(data_set)\n",
        "\n",
        "src_train_dataset, src_test_dataset,info = src_train_dataset_0, src_test_dataset_0,info_0\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndtoCeN3XfVC"
      },
      "source": [
        "## 2B. Create Train and Test dataset splits \n",
        "\n",
        "Note that the images from the stanford dog and many other datasets are not uniform in size. The `gen_datasets` function calls other routines from helper.py that scale the images from the dataset so the longest side fits into the 224 x 224 input window size of VGG16. The aspect ratio of the image is preserved, so the shorter side of the image is padded with zeros. The shorter side is randomly translated providing some data augmentation. (Augmentation is not important for this task). The routines also scale ground truth bounding boxes to match the scale and translation of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhuPmVI8_-op"
      },
      "source": [
        "train_dataset, test_dataset =helper.gen_datasets(data_set,src_train_dataset,src_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G75Q5CYU__ac"
      },
      "source": [
        "Display Some Training Images with Ground truth Bounding Box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib3p2gAAA9MS"
      },
      "source": [
        "\n",
        "helper.display_dataset_img(train_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w28N1Uv-NFg"
      },
      "source": [
        "## 3A. Import and Setup VGG model\n",
        "For this assignment we will use the convolutional layers of the VGG-16 module. The sturucture & layer names of the VGG-16 can be viewed on [Netscope](https://ethereon.github.io/netscope/#/preset/vgg-16). The layer_names variable allows selection of the output layer(s) of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sumqR_v1kC-W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9uiymlY-MGd"
      },
      "source": [
        "\n",
        "# Load base model\n",
        "def base_vgg_model(layer_names = ['block5_conv3']):\n",
        "   IMG_SHAPE = (HW_trg, HW_trg, 3)\n",
        "   base_vgg_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                             include_top=False,\n",
        "                                             weights='imagenet')\n",
        "   \n",
        "   op_list=[base_vgg_model.get_layer(layer).output for layer in layer_names]\n",
        "   base_model= tf.keras.Model(inputs=base_vgg_model.input, outputs=op_list)\n",
        "\n",
        "   return base_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOLjyxvmGgEK"
      },
      "source": [
        "# Select Output Layers\n",
        "layer_names = ['block4_conv3', 'block5_conv3', ]\n",
        "\n",
        "base_model = base_vgg_model(layer_names=layer_names)\n",
        "base_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-1FX4FuwxSn"
      },
      "source": [
        "## 3B. Extract output features maps from Model.\n",
        "\n",
        "The function `gen_batch_features` in the code cell below applies `n_img` images from the test or training set to the model. The function returns batch tensors of the image, ground truth boxes and features. The 0 dimension of the tensor corresponds to each image/feature pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY6rDPdPIK0c"
      },
      "source": [
        "\n",
        "def gen_batch_features(base_model, train_dataset,n_img):\n",
        "\n",
        "    cntr = 0\n",
        "    for img, boxes, obj_cen, labels in train_dataset.take(n_img):\n",
        "       img = img.to_tensor(shape=[batch_size, HW_trg, HW_trg, 3])\n",
        "       A = base_model(img)[0]\n",
        "       B = base_model(img)[1]\n",
        "\n",
        "       # Append output features \n",
        "       if cntr >0:\n",
        "          op_features_A = tf.concat([op_features_A,A],axis=0)\n",
        "          op_features_B = tf.concat([op_features_B,B],axis=0)\n",
        "\n",
        "          img_batch = tf.concat([img_batch,img],axis=0)\n",
        "          boxes_batch = tf.concat([boxes_batch,boxes],axis=0)\n",
        "       else:   \n",
        "          op_features_A= A\n",
        "          op_features_B= B\n",
        "\n",
        "          img_batch = img\n",
        "          boxes_batch = boxes\n",
        "       cntr +=1  \n",
        "     \n",
        "    return img_batch,boxes_batch,op_features_A, op_features_B   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJAKzVqO4Zi"
      },
      "source": [
        "n_img=100 # Default value.\n",
        "\n",
        "img_batch_0,     batch_boxes_0,     op_features_00,      op_features_01     =gen_batch_features(base_model, train_dataset,n_img)\n",
        "img_batch_test_0,batch_boxes_test_0,op_features_test_00, op_features_test_01=gen_batch_features(base_model, test_dataset,n_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY_Nd7Iv-L13"
      },
      "source": [
        "### Algorithm 1 Finding the largest connected component\n",
        "Require: The resized indicator matrix P 1 corresponding to\n",
        "an image I;\n",
        "1.   Transform P 1 into a binary map ˆP 1,\n",
        "where ˆp1(i,j) =\n",
        "{1 if p1(i,j) > 0\n",
        "0 otherwise ;\n",
        "2.   Select one pixel p in ˆP 1 as the starting point;\n",
        "3.   while True do\n",
        "4.   Use a flood-fill algorithm to label all the pixels in the\n",
        "connected component containing p;\n",
        "5.   if All the pixels are labeled then\n",
        "6.   Break;\n",
        "7.   end if\n",
        "8.   Search for the next unlabeled pixel as p;\n",
        "9.    end while\n",
        "10.   Obtain the connectivity of the connected components,\n",
        "and their corresponding size (pixel numbers);\n",
        "11.   Select the connected component ˆP 1c with the largest pixel\n",
        "number;\n",
        "12.   return The largest connected component ˆP 1c .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Algorithm 2 Deep Descriptor Transforming (DDT)\n",
        "**Require:** A set of N images containing the common object,\n",
        "and a pre-trained CNN model F;\n",
        "\n",
        "\n",
        "1.    Feed these images with their original resolutions into F;\n",
        "\n",
        "2.   Collect the corresponding convolutional descriptors X1, . . . , XN from the last convolutional layer of F;\n",
        "\n",
        "3.   Calculate the mean vector  ̄x of all the descriptors using\n",
        "Eq. 1;\n",
        " \n",
        "4.   Compute the covariance matrix Cov(x) of these deep\n",
        "descriptors based on Eq. 2;\n",
        "5.   Compute the eigenvectors ξ1, . . . , ξd of Cov(x);\n",
        "6.   Select ξ1 with the largest eigenvalue as the main trans-\n",
        "forming direction;\n",
        "7.   **repeat**\n",
        "    \n",
        "    7.1.   Calculate the indicator matrix P 1 for image I based\n",
        "on Eq. 3 and Eq. 4;\n",
        "\n",
        "    7.2.   Resize P 1 into its image’s resolution by nearest inter-\n",
        "polation;\n",
        "\n",
        "    7.3.   Collect the largest connected component ˆP 1c of these\n",
        "positive regions of the resized P 1 by Algo. 1;\n",
        "\n",
        "    7.4.   Obtain the minimum rectangle bounding box coveringˆP 1c as the prediction;\n",
        "\n",
        "8.   **until** All the N images are done;\n",
        "9.   return The minimum rectangle bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nhwxol0bdRr"
      },
      "source": [
        "## 4. DDT Algorithm\n",
        "\n",
        "In the code cells below add your implementation of the DDT Algorithm for object co-localisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTJVve6qjbzr"
      },
      "source": [
        "def describe_features(op_features, verbose=True):\n",
        "  fs = np.shape(op_features)\n",
        "  if verbose:\n",
        "    print(f\"{fs[0]} features of {str(fs[1])}{str(''.join(map(str, ['x'+str(i) for i in fs[2:]])))} shape\")\n",
        "  return fs\n",
        "\n",
        "def reshape_features(op_features, verbose=True):\n",
        "  fs = describe_features(op_features, verbose=verbose)\n",
        "  fs_0 = numpy.prod(fs[:len(fs)-1])\n",
        "  if verbose:\n",
        "    print(f\"reshaping to {fs_0} ({str(''.join(map(str, [str(i)+'x' for i in fs[:len(fs)-2]])))}{str(fs[1])}) samples of {fs[-1]}\")\n",
        "  return tf.reshape(op_features, [fs_0, fs[-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ehp2NSutGZ"
      },
      "source": [
        "def print_shapes(lists):\n",
        "  print(\"Image Shape Transformations:\")\n",
        "  for l in lists:\n",
        "    if isinstance(l, str):\n",
        "      msg=''\n",
        "      if l[0]=='7':\n",
        "        msg = \"\\nstep: \"\n",
        "      print(msg+l)\n",
        "    elif isinstance(l,(list,pd.core.series.Series,np.ndarray)) or tf.is_tensor(l):\n",
        "      print(np.shape(l))\n",
        "\n",
        "def find_largest_connected_component(image_nearest_interpolation):\n",
        "  #A1\n",
        "  # 1 -> 9\n",
        "  binary_img = cv2.threshold(np.uint8(image_nearest_interpolation), 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  # 10  - Obtain the connectivity of the connected components, and their corresponding size (pixel numbers);\n",
        "  nb_components, output, stats, centroids  = cv2.connectedComponentsWithStats(binary_img, cv2.CV_32S, connectivity=8)\n",
        "  # print((np.shape(numLabels), np.shape(labels), np.shape(stats), np.shape(centroids)))\n",
        "\n",
        "  # 11  - The largest connected component ˆP1c .\n",
        "  largest_connected_component_0 = stats[np.argsort(stats[:, cv2.CC_STAT_AREA])][0]\n",
        "  largest_connected_component_1 = stats[np.argsort(stats[:, cv2.CC_STAT_AREA])][1]\n",
        "\n",
        "  #A1 end\n",
        "  return nb_components, output, stats,  largest_connected_component_0, largest_connected_component_1, binary_img\n",
        "\n",
        "\n",
        "\n",
        "def define_bounding_box(nb_components, largest_connected_component):\n",
        "  if nb_components>0:\n",
        "    stat_left   = largest_connected_component[cv2.CC_STAT_LEFT]/255 \n",
        "    stat_top    = largest_connected_component[cv2.CC_STAT_TOP]/255 \n",
        "    stat_right  = largest_connected_component[cv2.CC_STAT_WIDTH]/255 + stat_left\n",
        "    stat_bottom = largest_connected_component[cv2.CC_STAT_HEIGHT]/255 + stat_top\n",
        "    return tf.constant([[stat_left , stat_top, stat_bottom, stat_right]])\n",
        "  else:\n",
        "    return tf.constant([[0,0,0,0]])\n",
        "\n",
        "\n",
        "\n",
        "def get_indicator_matrix(pca, op_feat, feature_transformation_shape_logs):\n",
        "    op_feats_reshaped = reshape_features(op_feat, verbose=False)\n",
        "    feature_transformation_shape_logs.extend([\"7.1\", \"\\nop_feats_reshaped\", op_feats_reshaped])\n",
        "\n",
        "    pflat = pca.transform(op_feats_reshaped)\n",
        "    feature_transformation_shape_logs.extend([\"\\npflat\", pflat])\n",
        "\n",
        "    indicator_matrix_14x14_0 = tf.reshape(pflat, [np.shape(op_feat)[0], np.shape(op_feat)[1]])\n",
        "    feature_transformation_shape_logs.extend([\"\\nindicator_matrix_14x14_0\", indicator_matrix_14x14_0])\n",
        "\n",
        "    #  just to keep images being 3D tensors (https://stackoverflow.com/a/58997339/12279207)\n",
        "    indicator_matrix_14x14_as3D_tensor = indicator_matrix_14x14_0[..., tf.newaxis]\n",
        "    feature_transformation_shape_logs.extend([\"\\nindicator_matrix_14x14_1\", indicator_matrix_14x14_as3D_tensor])\n",
        "\n",
        "\n",
        "    return indicator_matrix_14x14_as3D_tensor, feature_transformation_shape_logs\n",
        "  \n",
        "\n",
        "def run_pca(pca, op_feats):\n",
        "  bounding_boxes_0=[]\n",
        "  bounding_boxes_1=[]\n",
        "\n",
        "  binary_imgs = []\n",
        "  indicator_matrices = []\n",
        "  feature_transformation_shape_logs = [op_feats]\n",
        "\n",
        "  first=True\n",
        "  for i in range(len(img_batch_test)):\n",
        "  #A2\n",
        "    #7.1 - Calculate the indicator matrix P 1 for image\n",
        "    indicator_matrix_14x14_1, feature_transformation_shape_logs = get_indicator_matrix(pca, op_feats[i], feature_transformation_shape_logs)\n",
        "\n",
        "    # 7.2  - Resize P 1 into its image’s resolution by nearest interpolation;\n",
        "    image_nearest_interpolation = tf.image.resize(indicator_matrix_14x14_1, [224, 224], preserve_aspect_ratio=True, antialias=False, name=None\n",
        "                                                  ).numpy().astype(np.uint8)\n",
        "\n",
        "    feature_transformation_shape_logs.extend([\"7.2\", \"\\nimage_nearest_interpolation\", image_nearest_interpolation])\n",
        "    indicator_matrices.append(tf.reshape(image_nearest_interpolation, [224,224], name=None))\n",
        "\n",
        "\n",
        "    image_nearest_interpolation = cv2.normalize(src=image_nearest_interpolation,dst=None,alpha=0,beta=255,norm_type=cv2.NORM_MINMAX,dtype=cv2.CV_8U)\n",
        "    image_nearest_interpolation = tf.clip_by_value(image_nearest_interpolation, clip_value_min=0, clip_value_max=255)\n",
        "\n",
        "\n",
        "    #7.3  - Collect the largest connected component ˆP1c of these positive regions of the resized P1 by Algorithm 1\n",
        "    (nb_components, output, stats, \n",
        "     largest_connected_component_0, largest_connected_component_1, binary_img) = find_largest_connected_component(image_nearest_interpolation)\n",
        "    binary_imgs.append(binary_img)\n",
        "    # 7.4   - Obtain the minimum rectangle bounding box\n",
        "    bounding_boxes_0.append(\n",
        "        define_bounding_box(nb_components, largest_connected_component_0))\n",
        "    bounding_boxes_1.append(\n",
        "        define_bounding_box(nb_components, largest_connected_component_1))\n",
        "\n",
        "    # 9: return The minimum rectangle bounding boxes    \n",
        "    if first:\n",
        "      print_shapes(feature_transformation_shape_logs)\n",
        "      first=False\n",
        "  return [bounding_boxes_0, bounding_boxes_1], binary_imgs, indicator_matrices\n",
        "\n",
        "def Object_coLocalisation(op_features, op_features_test):\n",
        "  pca_dogs_model = PCA(n_components=1)\n",
        "  op_features_reshaped = reshape_features(op_features)\n",
        "  pca_dogs = pca_dogs_model.fit_transform(op_features_reshaped)\n",
        "  bounding_boxes_DDT, binary_imgs_DDT, indicator_matrices_DDT = run_pca(pca_dogs_model, op_features_test)\n",
        "  return bounding_boxes_DDT, binary_imgs_DDT, indicator_matrices_DDT\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYGjf2JDNKG2"
      },
      "source": [
        "(bounding_boxes_DDT_0,\n",
        " binary_imgs_DDT_0,\n",
        " indicator_matrices_DDT_0) = Object_coLocalisation(op_features_00, op_features_test_00)\n",
        "\n",
        "(bounding_boxes_DDT_1,\n",
        " binary_imgs_DDT_1,\n",
        " indicator_matrices_DDT_1) = Object_coLocalisation(op_features_01, op_features_test_01)\n",
        "\n",
        "\n",
        "bounding_boxes_DDT_0_01 = [bounding_boxes_DDT_0, bounding_boxes_DDT_1]\n",
        "print(np.shape(bounding_boxes_DDT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHkzgNgBrRyw"
      },
      "source": [
        "## 5. Display results\n",
        "\n",
        "You can display your results using the code cell below. You can display an image(s) similar to that shown at the start of the notebook, containing the image, ground truth bounding box and the bounding box produced by the DDT method. You can also show the binary map produced by indicator P (positive values).\n",
        "\n",
        "The function `helper.image_with_gt_boxes(img,boxes,colour)` can be used to plot a bounding box on an image where the bounding box is in the form [ymin,xmin,ymax,xmax] and values are normalised between zero and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVIoiJX9QiJX"
      },
      "source": [
        "def draw_bounds(ax, curr_img, title):\n",
        "  # Plot image and show related indicator\n",
        "  ax.axis('off')\n",
        "  ax.set_title(title)\n",
        "  ax.imshow(curr_img)\n",
        "\n",
        "def display_img_results():\n",
        "  ground_truth_img=helper.image_with_gt_boxes(curr_img,batch_boxes_test[k],(0,255,0))\n",
        "  ax = axs[0][k]\n",
        "  draw_bounds(ax, ground_truth_img, \"Ground Truth\")\n",
        "\n",
        "\n",
        "def display_image_bounding_box_results(k, axs, img_batch_test, batch_boxes_test, bounding_boxes_DDT):\n",
        "  curr_img = np.asarray(helper.unprocess_image(img_batch_test[k, :, :,:]))\n",
        "  curr_img=helper.image_with_gt_boxes(curr_img,batch_boxes_test[k],(0,255,0))\n",
        "  draw_bounds(axs[0][k], curr_img, \"Ground Truth\")\n",
        "\n",
        "  for bbs in bounding_boxes_DDT:\n",
        "\n",
        "    for color, box_type in zip([(255,0,0),(245,218,223)], bbs):\n",
        "      curr_img=helper.image_with_gt_boxes(curr_img, box_type[k], color)\n",
        "      draw_bounds(axs[1][k], curr_img, \"Prediction\")\n",
        "\n",
        "def display_All_coLocalisation_results(img_batch_test, batch_boxes_test, bounding_boxes_DDT, indicator_matrices_DDT, binary_imgs_DDT, n=8):\n",
        "  n=8\n",
        "  fig, axs = plt.subplots(4,n, figsize=(30, 15))\n",
        "\n",
        "  # Iterate through some images\n",
        "  for k in range(n):\n",
        "    display_image_bounding_box_results(k, axs[:2], img_batch_test, batch_boxes_test, bounding_boxes_DDT)\n",
        "    axs[2][k].imshow(indicator_matrices_DDT[k], cmap='gray')\n",
        "    axs[2][k].set_title(\"indicator_matrix\")\n",
        "    axs[2][k].axis('off')\n",
        "    axs[3][k].imshow(binary_imgs_DDT[k], cmap='gray')\n",
        "    axs[3][k].set_title(\"binary_img\")\n",
        "    axs[3][k].axis('off')\n",
        "\n",
        "display_All_coLocalisation_results(img_batch_test_0, batch_boxes_test_0, bounding_boxes_DDT_0_01, indicator_matrices_DDT_0, binary_imgs_DDT_0, n=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsQ_FnNWU-nX"
      },
      "source": [
        "## DDT+ Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXogbDAYNSX2"
      },
      "source": [
        "data_set = ['voc', 16]\n",
        "try:\n",
        "  src_train_dataset_1, src_test_dataset_1,info_1\n",
        "except:\n",
        "  src_train_dataset_1, src_test_dataset_1,info_1 = get_data(data_set)\n",
        "\n",
        "src_train_dataset, src_test_dataset,info = src_train_dataset_1, src_test_dataset_1,info_1\n",
        "\n",
        "train_dataset, test_dataset =helper.gen_datasets(data_set,src_train_dataset,src_test_dataset)\n",
        "\n",
        "helper.display_dataset_img(train_dataset)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "untDeOKjeUne"
      },
      "source": [
        "n_img=100 # Default value.\n",
        "\n",
        "\n",
        "\n",
        "img_batch_1,     batch_boxes_1,     op_features_10,      op_features_11     =gen_batch_features(base_model, train_dataset,n_img)\n",
        "img_batch_test_1,batch_boxes_test_1,op_features_test_10, op_features_test_11=gen_batch_features(base_model, test_dataset,n_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6AV-RWreVi2"
      },
      "source": [
        "(bounding_boxes_DDT_0,\n",
        " binary_imgs_DDT_0,\n",
        " indicator_matrices_DDT_0) = Object_coLocalisation(op_features_10, op_features_test_10)\n",
        "\n",
        "(bounding_boxes_DDT_1,\n",
        " binary_imgs_DDT_1,\n",
        " indicator_matrices_DDT_1) = Object_coLocalisation(op_features_11, op_features_test_11)\n",
        "\n",
        "\n",
        "bounding_boxes_DDT_1_01 = [bounding_boxes_DDT_0, bounding_boxes_DDT_1]\n",
        "print(np.shape(bounding_boxes_DDT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z1JfYvVeV-t"
      },
      "source": [
        "# fig = plt.figure(figsize=(15, 12))\n",
        "display_All_coLocalisation_results(img_batch_test_1, batch_boxes_test_1, bounding_boxes_DDT_1_01, indicator_matrices_DDT_0, binary_imgs_DDT_0, n=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR7CnnEmeWV2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNXn5U6eeWtd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_xo4xTKeXDd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYrzfDHseXdt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}